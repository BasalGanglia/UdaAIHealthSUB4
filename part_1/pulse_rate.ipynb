{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken more or less directly from the course material for ECG\n",
    "# From the README we get that sampling rate is 125\n",
    "\n",
    "def BandpassFilter(signal, low_th, high_th, fs=125):\n",
    "    \"\"\"Bandpass filter\n",
    "\n",
    "    Args:\n",
    "        signal: (np.array) the signal to be filtered.\n",
    "        low_th: (np.array) low threshold for the filter.\n",
    "        high_th: (np.array) high threshold for filter.\n",
    "        fs: (number) the sampling rate of the signal\n",
    "\n",
    "    Returns:\n",
    "        filtered signal\n",
    "    \"\"\"\n",
    "\n",
    "    b, a = sp.signal.butter(3, (low_th,high_th), btype='bandpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b,a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I find the description of \"confidence\" in this project rather confusing, but will just use the SNR idea\n",
    "# that was presented in the course material. However, it does not take into account the fact that the movement\n",
    "# artifacts and the PPG signal happen at same power band...\n",
    "def CalcSNR(sig, hr_bpm):\n",
    "    \"\"\" Calculate signal-to-noise ratio\n",
    "\n",
    "    Args:\n",
    "        signal: (np.array) the signal to be analyzed\n",
    "        hr_bpm: the HR from which the signal frequency is estimated\n",
    "\n",
    "    Returns:\n",
    "        signal-to-noise ratio\n",
    "    \"\"\"\n",
    "\n",
    "    # Our data is sampled at 125 Hz\n",
    "    fs = 125\n",
    "\n",
    "    hr_f = hr_bpm / 60\n",
    "\n",
    "    harmonic_f = hr_f * 2\n",
    "\n",
    "    low_thr = 40/60\n",
    "    high_thr = 240/ 60\n",
    "    sig = BandpassFilter(sig, low_thr, high_thr, fs)\n",
    "    \n",
    "    # found this in a knowledge base example:\n",
    "    # https://knowledge.udacity.com/questions/314330\n",
    "    n = len(sig) * 3\n",
    "\n",
    "    freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "    fft = np.abs(np.fft.rfft(sig,n))\n",
    "    fft[freqs <= 40/60.0] = 0.0\n",
    "    fft[freqs >= 240/60.0] = 0.0\n",
    "\n",
    "    \n",
    "    est_fs = hr_f / 55.0\n",
    "    fs_win = 30  / 60.0\n",
    "    fs_win_e = (freqs >= est_fs - fs_win) & (freqs <= est_fs +fs_win)\n",
    "    conf = np.sum(fft[fs_win_e])/np.sum(fft)\n",
    "        \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adapted from course material....\n",
    "\n",
    "def Featurize(accx, accy, accz, ppg, fs):\n",
    "  \"\"\"Featurization of the accelerometer signal.\n",
    "\n",
    "  Args:\n",
    "      accx: (np.array) x-channel of the accelerometer.\n",
    "      accy: (np.array) y-channel of the accelerometer.\n",
    "      accz: (np.array) z-channel of the accelerometer.\n",
    "      fs: (number) the sampling rate of the accelerometer\n",
    "\n",
    "  Returns:\n",
    "      n-tuple of accelerometer features\n",
    "  \"\"\"\n",
    "\n",
    "  low_thr = 40/60\n",
    "  high_thr = 240/ 60\n",
    "  accx = BandpassFilter(accx, low_thr, high_thr, fs)\n",
    "  accy = BandpassFilter(accy,low_thr, high_thr, fs)\n",
    "  accz = BandpassFilter(accz,low_thr, high_thr, fs)\n",
    "  ppg = BandpassFilter(ppg,low_thr, high_thr, fs)\n",
    "\n",
    "\n",
    "  # The mean of each channel\n",
    "  mn_x = np.mean(accx)\n",
    "  mn_y = np.mean(accy)\n",
    "  mn_z = np.mean(accz)\n",
    "  mn_p= np.mean(ppg)\n",
    "\n",
    "\n",
    "  # The standard deviation of each channel\n",
    "  std_x = np.std(accx)\n",
    "  std_y = np.std(accy)\n",
    "  std_z = np.std(accz)\n",
    "  std_p = np.std(ppg)\n",
    "\n",
    "  # Various percentile values for each channel\n",
    "  p5_x = np.percentile(accx, 5)\n",
    "  p5_y = np.percentile(accy, 5)\n",
    "  p5_z = np.percentile(accz, 5)\n",
    "  p5_p = np.percentile(ppg, 5)\n",
    "  p10_x = np.percentile(accx, 10)\n",
    "  p10_y = np.percentile(accy, 10)\n",
    "  p10_z = np.percentile(accz, 10)\n",
    "  p10_p = np.percentile(ppg, 10)\n",
    "  p25_x = np.percentile(accx, 25)\n",
    "  p25_y = np.percentile(accy, 25)\n",
    "  p25_z = np.percentile(accz, 25)\n",
    "  p25_p = np.percentile(ppg, 25)\n",
    "  p50_x = np.percentile(accx, 50)\n",
    "  p50_y = np.percentile(accy, 50)\n",
    "  p50_z = np.percentile(accz, 50)\n",
    "  p50_p = np.percentile(ppg, 50)\n",
    "  p90_x = np.percentile(accx, 90)\n",
    "  p90_y = np.percentile(accy, 90)\n",
    "  p90_z = np.percentile(accz, 90)\n",
    "  p90_p = np.percentile(ppg, 90)\n",
    "\n",
    "  # The pearson correlation of all pairs of channels\n",
    "  corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "  corr_xz = sp.stats.pearsonr(accx, accz)[0]\n",
    "  corr_yz = sp.stats.pearsonr(accy, accz)[0]\n",
    "  corr_py = sp.stats.pearsonr(ppg, accy)[0]\n",
    "  corr_px = sp.stats.pearsonr(ppg, accx)[0]\n",
    "  corr_pz = sp.stats.pearsonr(ppg, accz)[0]\n",
    "\n",
    "\n",
    "\n",
    "  # The total energy of each channel\n",
    "  energy_x = np.sum(np.square(accx - np.mean(accx)))\n",
    "  energy_y = np.sum(np.square(accy - np.mean(accy)))\n",
    "  energy_z = np.sum(np.square(accz - np.mean(accz)))\n",
    "  energy_p = np.sum(np.square(ppg - np.mean(ppg)))\n",
    "\n",
    "\n",
    "  # Take an FFT of the signal. If the signal is too short, 0-pad it so we have \n",
    "  # at least 2046 points in the FFT.\n",
    "  fft_len = max(len(accx), 2046)\n",
    "\n",
    "  # Create an array of frequency bins\n",
    "  fft_freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "\n",
    "  # Helper function to select frequency bins between <low> and <high>\n",
    "  freqs_bw = lambda low, high: (fft_freqs >= low) & (fft_freqs <= high)\n",
    "\n",
    "  # Compute the accelerometer magnitude\n",
    "  accm = np.sqrt(np.sum(np.square(np.vstack((accx, accy, accz))), axis=0))\n",
    "\n",
    "  # Take an FFT of the centered signal\n",
    "  fft_x = np.fft.rfft(accx, fft_len)\n",
    "  fft_y = np.fft.rfft(accy, fft_len)\n",
    "  fft_z = np.fft.rfft(accz, fft_len)\n",
    "  fft_m = np.fft.rfft(accm, fft_len)\n",
    "  fft_p = np.fft.rfft(ppg, fft_len)\n",
    "\n",
    "\n",
    "  # Compute the energy spectrum\n",
    "  spec_energy_x = np.square(np.abs(fft_x))\n",
    "  spec_energy_y = np.square(np.abs(fft_y))\n",
    "  spec_energy_z = np.square(np.abs(fft_z))\n",
    "  spec_energy_m = np.square(np.abs(fft_m))\n",
    "  spec_energy_p = np.square(np.abs(fft_p))\n",
    "\n",
    "  # The frequency with the most power between 0.25 and 12 Hz\n",
    "  dom_x = fft_freqs[np.argmax(fft_x[freqs_bw(0.25, 12)])]\n",
    "  dom_y = fft_freqs[np.argmax(fft_y[freqs_bw(0.25, 12)])]\n",
    "  dom_z = fft_freqs[np.argmax(fft_z[freqs_bw(0.25, 12)])]\n",
    "  dom_m = fft_freqs[np.argmax(fft_m[freqs_bw(0.25, 12)])]\n",
    "  dom_p = fft_freqs[np.argmax(fft_p[freqs_bw(0.25, 12)])]\n",
    "\n",
    "\n",
    "  # The fraction of energy in various frequency bins for each channel\n",
    "  energy_01_x = (np.sum(spec_energy_x[freqs_bw(0, 1)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_01_y = (np.sum(spec_energy_x[freqs_bw(0, 1)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_01_z = (np.sum(spec_energy_x[freqs_bw(0, 1)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_01_m = (np.sum(spec_energy_x[freqs_bw(0, 1)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_01_p = (np.sum(spec_energy_x[freqs_bw(0, 1)]) \n",
    "                 / np.sum(spec_energy_p))\n",
    "\n",
    "\n",
    "  \n",
    "  energy_12_x = (np.sum(spec_energy_x[freqs_bw(1, 2)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_12_y = (np.sum(spec_energy_x[freqs_bw(1, 2)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_12_z = (np.sum(spec_energy_x[freqs_bw(1, 2)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_12_m = (np.sum(spec_energy_x[freqs_bw(1, 2)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_12_p = (np.sum(spec_energy_x[freqs_bw(1, 2)]) \n",
    "                 / np.sum(spec_energy_p))\n",
    "  energy_23_x = (np.sum(spec_energy_x[freqs_bw(2, 3)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_23_y = (np.sum(spec_energy_x[freqs_bw(2, 3)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_23_z = (np.sum(spec_energy_x[freqs_bw(2, 3)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_23_p = (np.sum(spec_energy_x[freqs_bw(2, 3)]) \n",
    "                 / np.sum(spec_energy_p)) \n",
    "  energy_23_m = (np.sum(spec_energy_x[freqs_bw(2, 3)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_34_x = (np.sum(spec_energy_x[freqs_bw(3, 4)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_34_y = (np.sum(spec_energy_x[freqs_bw(3, 4)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_34_z = (np.sum(spec_energy_x[freqs_bw(3, 4)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_34_m = (np.sum(spec_energy_x[freqs_bw(3, 4)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_34_p = (np.sum(spec_energy_x[freqs_bw(3, 4)]) \n",
    "                 / np.sum(spec_energy_p))\n",
    "  energy_45_x = (np.sum(spec_energy_x[freqs_bw(4, 5)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_45_y = (np.sum(spec_energy_x[freqs_bw(4, 5)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_45_z = (np.sum(spec_energy_x[freqs_bw(4, 5)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_45_m = (np.sum(spec_energy_x[freqs_bw(4, 5)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_45_p = (np.sum(spec_energy_x[freqs_bw(4, 5)]) \n",
    "                 / np.sum(spec_energy_p))\n",
    "  energy_56_x = (np.sum(spec_energy_x[freqs_bw(5, 6)]) \n",
    "                 / np.sum(spec_energy_x))\n",
    "  energy_56_y = (np.sum(spec_energy_x[freqs_bw(5, 6)]) \n",
    "                 / np.sum(spec_energy_y))\n",
    "  energy_56_z = (np.sum(spec_energy_x[freqs_bw(5, 6)]) \n",
    "                 / np.sum(spec_energy_z))\n",
    "  energy_56_m = (np.sum(spec_energy_x[freqs_bw(5, 6)]) \n",
    "                 / np.sum(spec_energy_m))\n",
    "  energy_56_p = (np.sum(spec_energy_x[freqs_bw(5, 6)]) \n",
    "                 / np.sum(spec_energy_p))\n",
    "  \n",
    "\n",
    "  return (mn_x,\n",
    "          mn_y,\n",
    "          mn_z,\n",
    "          mn_p,\n",
    "          std_x,\n",
    "          std_y,\n",
    "          std_z,\n",
    "          std_p,\n",
    "          p5_x,\n",
    "          p5_y,\n",
    "          p5_z,\n",
    "          p5_p,\n",
    "          p10_x,\n",
    "          p10_y,\n",
    "          p10_z,\n",
    "          p10_p,\n",
    "          p25_x,\n",
    "          p25_y,\n",
    "          p25_z,\n",
    "          p25_p,\n",
    "          p50_x,\n",
    "          p50_y,\n",
    "          p50_z,\n",
    "          p50_z,\n",
    "          p90_x,\n",
    "          p90_y,\n",
    "          p90_z,\n",
    "          p90_p,\n",
    "          corr_xy,\n",
    "          corr_xz,\n",
    "          corr_yz,\n",
    "          corr_px,\n",
    "          corr_py,\n",
    "          corr_pz,\n",
    "          energy_x,\n",
    "          energy_y,\n",
    "          energy_z,\n",
    "          energy_p,\n",
    "          dom_x,\n",
    "          dom_y,\n",
    "          dom_z,\n",
    "          dom_m,\n",
    "          dom_p,\n",
    "          energy_01_x,\n",
    "          energy_12_x,\n",
    "          energy_23_x,\n",
    "          energy_34_x,\n",
    "          energy_45_x,\n",
    "          energy_56_x,\n",
    "          energy_01_y,\n",
    "          energy_12_y,\n",
    "          energy_23_y,\n",
    "          energy_34_y,\n",
    "          energy_45_y,\n",
    "          energy_56_y,\n",
    "          energy_01_z,\n",
    "          energy_12_z,\n",
    "          energy_23_z,\n",
    "          energy_34_z,\n",
    "          energy_45_z,\n",
    "          energy_56_z,\n",
    "          energy_01_m,\n",
    "          energy_12_m,\n",
    "          energy_23_m,\n",
    "          energy_34_m,\n",
    "          energy_45_m,\n",
    "          energy_56_m,\n",
    "          energy_01_p,\n",
    "          energy_12_p,\n",
    "          energy_23_p,\n",
    "          energy_34_p,\n",
    "          energy_45_p,\n",
    "          energy_56_p, \n",
    "          )\n",
    "\n",
    "def FeatureNames():\n",
    "  \"\"\"Returns the names of all the features.\"\"\"\n",
    "  return ('mn_x',\n",
    "          'mn_y',\n",
    "          'mn_z',\n",
    "          'mn_p',\n",
    "          'std_x',\n",
    "          'std_y',\n",
    "          'std_z',\n",
    "          'std_p',\n",
    "          'p5_x',\n",
    "          'p5_y',\n",
    "          'p5_z',\n",
    "          'p5_p',\n",
    "          'p10_x',\n",
    "          'p10_y',\n",
    "          'p10_z',\n",
    "          'p10_p',\n",
    "          'p25_x',\n",
    "          'p25_y',\n",
    "          'p25_z',\n",
    "          'p25_p',\n",
    "          'p50_x',\n",
    "          'p50_y',\n",
    "          'p50_z',\n",
    "          'p50_p',\n",
    "          'p90_x',\n",
    "          'p90_y',\n",
    "          'p90_z',\n",
    "          'p90_p',\n",
    "          'corr_xy',\n",
    "          'corr_xz',\n",
    "          'corr_yz',\n",
    "          'corr_px',\n",
    "          'corr_py',\n",
    "          'coor_pz',\n",
    "          'energy_x',\n",
    "          'energy_y',\n",
    "          'energy_z',\n",
    "          'energy_p',\n",
    "          'dom_x',\n",
    "          'dom_y',\n",
    "          'dom_z',\n",
    "          'dom_m',\n",
    "          'dom_p',\n",
    "          'energy_01_x',\n",
    "          'energy_12_x',\n",
    "          'energy_23_x',\n",
    "          'energy_34_x',\n",
    "          'energy_45_x',\n",
    "          'energy_56_x',\n",
    "          'energy_01_y',\n",
    "          'energy_12_y',\n",
    "          'energy_23_y',\n",
    "          'energy_34_y',\n",
    "          'energy_45_y',\n",
    "          'energy_56_y',\n",
    "          'energy_01_z',\n",
    "          'energy_12_z',\n",
    "          'energy_23_z',\n",
    "          'energy_34_z',\n",
    "          'energy_45_z',\n",
    "          'energy_56_z',\n",
    "          'energy_01_m',\n",
    "          'energy_12_m',\n",
    "          'energy_23_m',\n",
    "          'energy_34_m',\n",
    "          'energy_45_m',\n",
    "          'energy_56_m',\n",
    "          'energy_01_p',\n",
    "          'energy_12_p',\n",
    "          'energy_23_p',\n",
    "          'energy_34_p',\n",
    "          'energy_45_p',\n",
    "          'energy_56_p',\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_moving_average(data, window_size):\n",
    "    \"\"\" Calculate a moving average of a signal\n",
    "    args:\n",
    "        data: signal to be processed\n",
    "        window_size: the size of the MA window\n",
    "    returns:\n",
    "        filtered signal\n",
    "    \n",
    "    \"\"\"\n",
    "    ma_data = np.convolve(data, np.ones(window_size), \"same\") / window_size\n",
    "    return ma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTroikaRefFile(ref_fl):\n",
    "    \"\"\" Loads the reference data file.\n",
    "    args:\n",
    "        ref_fl : the reference data file\n",
    "    returns: \n",
    "        the reference HR \n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(ref_fl)\n",
    "    return data['BPM0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from re import I\n",
    "from xml.sax.handler import feature_external_ges\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs, preds = [], [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence, pred = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        preds.append(pred)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs), errs, confs, preds\n",
    "\n",
    "def ExtractFeatures(accx,accy,accz, ppg, fs=125, window=2):\n",
    "    \"\"\"Extract features from the dataset.\n",
    "\n",
    "    Args:\n",
    "        accx: accelerometer X axis signal\n",
    "        accy: accelerometer Y axis signal\n",
    "        accz: accelerometer Z axis signal\n",
    "        ppg: the ppg signal\n",
    "        fs: sampling rate\n",
    "        window: the size of window to extract from\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        features: list of features for each slice\n",
    "    \"\"\" \n",
    "    # Calculate the slice size (sz)\n",
    "    sz =fs*window\n",
    "    features = []\n",
    "\n",
    "    for i in range (0, len(ppg)- (fs*window), fs*2):\n",
    "        x = accx[i:i+sz]\n",
    "        y = accx[i:i+sz]\n",
    "        z = accz[i:i+sz]\n",
    "        p = ppg[i:i+sz]\n",
    "        features.append(Featurize(x,y,z,p, fs=fs))\n",
    "    return features\n",
    "\n",
    "import pickle\n",
    "def train_model():\n",
    "    \"\"\" Train a regression model to detect the HR\n",
    "\n",
    "    returns: \n",
    "        trained sklearn RF model \n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    features = [] \n",
    "    feature_names = FeatureNames() \n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "        ground_truth = LoadTroikaRefFile(ref_fl)\n",
    "        \n",
    "        feats = ExtractFeatures(accx, accy, accz, ppg, fs=125)\n",
    "        \n",
    "        lendiff = len(feats) - len(ground_truth)\n",
    "        if lendiff > 0:\n",
    "            feats = feats[lendiff:]\n",
    "        feat_df = pd.DataFrame(feats, columns = feature_names)\n",
    "        feat_df['y'] = ground_truth\n",
    "        features.append(feat_df)\n",
    "        \n",
    "    full_data = pd.concat(features)\n",
    "    rf = RandomForestRegressor(n_estimators=400,max_depth=16)\n",
    "\n",
    "    rf.fit(full_data.drop('y', axis=1), full_data['y'] )\n",
    "    pickle.dump(rf, open('tmp_model.pkl', 'wb'))\n",
    "    return rf\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    \"\"\" Run the algorithm on one file\n",
    "    args:\n",
    "        data_fl: File containing the data\n",
    "        ref_fl: file containing the reference HR\n",
    "    returns: \n",
    "        estimated error metrics\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # took some pointers from here https://knowledge.udacity.com/questions/314330\n",
    "    # Load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    ground_truth = LoadTroikaRefFile(ref_fl)\n",
    "\n",
    "    model = train_model()\n",
    "\n",
    "    # Extract features method takes care of the bandpass\n",
    "    # filtering  \n",
    "    feats = ExtractFeatures(accx, accy, accz, ppg, fs=125)\n",
    "\n",
    "    lendiff = len(feats) - len(ground_truth)\n",
    "    if lendiff > 0:\n",
    "        feats = feats[lendiff:]\n",
    "    \n",
    "    errors, confidence, preds = [], [], []\n",
    "    for i, feat in enumerate(feats):\n",
    "        pred = model.predict(np.reshape(feat, (1,-1)))[0]\n",
    "        snr = CalcSNR(ppg, pred)\n",
    "        errors.append(np.abs((pred-ground_truth[i][0])))\n",
    "        preds.append(pred)\n",
    "        confidence.append(snr)\n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "\n",
    "    smooth_errors = []\n",
    "    smooth_preds = my_moving_average(preds, window_size=5)\n",
    "    for i, predi in enumerate(smooth_preds):\n",
    "        smooth_errors.append(np.abs((predi- ground_truth[i][0])))\n",
    "\n",
    "   # # Return per-estimate mean absolute error and confidence as a 2-tuple of numpy arrays.\n",
    "    # errors, confidence = np.ones(100), np.ones(100)  # Dummy placeholders. Remove\n",
    "    return errors, confidence, smooth_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.981475270757212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "aggerr, errs, confs, preds = Evaluate()\n",
    "aggerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Description\n",
    "\n",
    "To run the code you just have to put the data and reference files into a folder where the LoadTroikaDataset method can find it. At the moment the code expects the data to be in .mat format, so in case you have data in another format, you must implement a customer loader function. \n",
    "\n",
    "# Data Description\n",
    "\n",
    "Data was problematic as the accelerometer signal was often fully obfuscating the PPG frequencies.\n",
    "\n",
    "# Algorithm Description\n",
    "\n",
    "The algorithm uses a black-box brute force method of calculation a ton of features and then training off-the-shelf random forest regression model to estimate the HR. It tries to take advantage of the fact that HR does not change too rapidly by smoothing the predictions. The algorithm outputs the mean average error over the whole training data, as well as also keeps tracking of some of the results for debugging purposes.\n",
    "\n",
    "There is a caveat in just looking at the MAE as it can hide outliers to some extend. Also, the algorithm output is now using only the data that the algorithm is \"confident\" about. However, as the confidence is based on the signal-to-noise ratio and does not take into account the fact that the accelometer is happening at the signal frequency, the current confidence estimate is pointless.\n",
    "The algorithm most likely fails if the amount of artifacts increases sufficiently.\n",
    "\n",
    "# Algorithm Performance\n",
    "\n",
    "The algorithm performed suprisingly well locally, giving average MAE of around 4 bmp. For some reason the errors were higher in the online unit test, but as it did not give any extra output, it was not possible to determine why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('VENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6647f40823f5c7708e74de4968345d59a5d594909a60d375a90a36ed74ab1aa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
